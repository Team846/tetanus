#!/usr/bin/env python3

"""Download static libraries and headers for wpilib + vendor depndencies.

Compiled libraries and headers are downloaded from each vendor's maven repository linked in
vendordeps JSON files.
"""

from datetime import datetime
from glob import glob
import json
import os
from os import path
import re
import shutil
import sys
from urllib import request

VENDORDEPS_DIR = "vendordeps"
INCLUDE_DIR = "include"
LIB_DIR = "lib"

WPILIB_DEP = "wpilib"
ROBORIO_DEP = "roborio"
VENDOR_DEPS = ["navx_frc", "Phoenix-latest", "REVColorSensorV3", "REVRobotics"]


def main():
    # Clean include and lib directories before downloading new deps
    for dir in [INCLUDE_DIR, LIB_DIR]:
        # Delete all contents except hidden files (doesn't delete .gitkeep)
        for f in os.listdir(dir):
            if f == ".gitkeep":
                continue

            p = os.path.join(dir, f)
            if path.isfile(p):
                os.remove(p)
            else:
                shutil.rmtree(p)

    # Download all wpilib deps
    with open(path.join(VENDORDEPS_DIR, f"{WPILIB_DEP}.json"), 'r') as wpilib_dep_file:
        j = json.load(wpilib_dep_file)
        for dep in j['dependencies']:
            install_dep(
                'wpilib',
                j['mavenUrl'],
                f"edu.wpi.first.{dep}",
                f"{dep}-cpp",
                j['version'],
                static=False,
                headers=True,
            )

    # Download all roborio runtime dpes
    with open(path.join(VENDORDEPS_DIR, f"{ROBORIO_DEP}.json"), 'r') as roborio_dep_file:
        j = json.load(roborio_dep_file)
        for dep in j['dependencies']:
            install_dep(
                dep['name'],
                j['mavenUrl'],
                "",
                dep['name'],
                dep['version'],
                static=False,
                headers=False,
            )

    # Download all vendor deps
    for vendor_dep in VENDOR_DEPS:
        with open(path.join(VENDORDEPS_DIR, f"{vendor_dep}.json"), 'r') as vendordep_file:
            j = json.load(vendordep_file)

            for dep in j['cppDependencies']:
                # Skip dependencies that arent compiled for roborio (probably simulation stuff)
                if "linuxathena" not in dep['binaryPlatforms']:
                    # print(f"Skipping {dep['groupId']}-{dep['artifactId']}")
                    continue

                install_dep(
                    j['name'],
                    j['mavenUrls'][0],
                    dep['groupId'],
                    dep['artifactId'],
                    dep['version'],
                    static=True,
                    headers=True
                )

    full_lib_dir = path.join(LIB_DIR, "linux", "athena", "shared")

    # Delete debug versions (e.g. libwpilibc.so.debug)
    for f in glob(path.join(full_lib_dir, f"*.debug")):
        os.remove(f)

    # Create symlinks to shared libraries with version numbers (e.g. libNiFpga.so -> libNiFpga.so.19.0.0)
    r = re.compile('(.*\.so)\..*')
    for f in os.listdir(full_lib_dir):
        m = r.match(f)
        if m:
            os.symlink(f, path.join(".", full_lib_dir, m.group(1)))

    # Delete misc files (notices, licenses, etc.)
    for dir in [INCLUDE_DIR, LIB_DIR]:
        for g in ["LICENSE.*", "RequiredVersion.txt", "ThirdPartyNotices.txt"]:
            for f in glob(path.join(dir, g)):
                os.remove(f)

    # Create autogenerated READMEs with timestamps
    for dir in [INCLUDE_DIR, LIB_DIR]:
        with open(f"{dir}/README.txt", "w") as f:
            f.write(f"Files downloaded by ../install_deps.py on {datetime.now()}")


def install_dep(vendor_name, maven_url, group_id, artifact_id, version, static, headers):
    """Download a dependency headers and static library."""

    url = urljoin(maven_url, group_id.replace('.', '/'), artifact_id, version)
    name = f"{artifact_id}-{version}"

    if headers:
        download_and_unzip(urljoin(url, f"{name}-headers.zip"), INCLUDE_DIR, name)

    download_and_unzip(
        urljoin(url, f"{name}-linuxathenastatic.zip" if static else f"{name}-linuxathena.zip"),
        LIB_DIR,
        name
    )


def download_and_unzip(url, dir, name):
    """Download a specificed zip file and unzip it."""

    full_path = path.join(dir, f"{name}.zip")

    print(f"Downloading {url}")
    try:
        request.urlretrieve(url, full_path)
        shutil.unpack_archive(full_path, dir)
        os.remove(full_path)
    except:
        sys.exit(f"Unable to download {url} - stopping")


def urljoin(*components):
    """Join url pieces together."""
    return '/'.join([c.strip('/') for c in components])


if __name__ == "__main__":
    main()
