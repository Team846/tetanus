#!/usr/bin/env python3

"""Download static libraries and headers for wpilib + vendor depndencies.

Compiled libraries and headers are downloaded from each vendor's maven repository linked in
vendordeps JSON files.

See vendordeps/README.md
"""

from datetime import datetime
from glob import glob
import json
import os
from os import path
import re
import shutil
import sys
from urllib import request

VENDOR_DEPS_DIR = "vendordeps"
WPILIB_DEPS_DIR = "vendordeps/wpilib"

INCLUDE_DIR = "include"
LIB_DIR = "lib"


def main():
    # Clean include and lib directories before downloading new deps
    for dir in [INCLUDE_DIR, LIB_DIR]:
        # Delete all contents except hidden files (except .gitkeep)
        for f in os.listdir(dir):
            if f == ".gitkeep":
                continue

            p = os.path.join(dir, f)
            if path.isfile(p):
                os.remove(p)
            else:
                shutil.rmtree(p)

    # Download wpilib related deps
    for vendordep in glob(path.join(WPILIB_DEPS_DIR, "*.json")):
        with open(vendordep, 'r') as f:
            j = json.load(f)
            for dep in j['dependencies']:
                # Files usually located in /<dep>/<dep>-<cpp>, except for ni-libraries
                group_id = dep if j['name'] != "ni-libraries" else ""
                artifact_id = f"{dep}-cpp" if j['name'] != "ni-libraries" else dep

                install_dep(
                    j['mavenUrl'],
                    group_id,
                    artifact_id,
                    j['version'],
                    static=False,
                    headers=j['headers'],
                )

    # Download vendor dependencies
    for vendordep in glob(path.join(VENDOR_DEPS_DIR, "*.json")):
        with open(vendordep, 'r') as f:
            j = json.load(f)
            for dep in j['cppDependencies']:
                # Skip dependencies that arent compiled for roborio (probably simulation stuff)
                if "linuxathena" not in dep['binaryPlatforms']:
                    continue

                install_dep(
                    j['mavenUrls'][0],
                    dep['groupId'],
                    dep['artifactId'],
                    dep['version'],
                    static=True,
                    headers=True
                )

    full_lib_dir = path.join(LIB_DIR, "linux", "athena", "shared")

    # Rename debug versions (e.g. libwpilibc.so.debug -> libwpilibcd.so)
    r = re.compile('(.*)\.so\.debug')
    for f in os.listdir(full_lib_dir):
        m = r.match(f)
        if m:
            os.rename(path.join(full_lib_dir, f), path.join(full_lib_dir, f"{m.group(1)}d.so"))

    # Create symlinks to shared libraries with version numbers (e.g. libNiFpga.so -> libNiFpga.so.19.0.0)
    r = re.compile('(.*\.so)\..*')
    for f in os.listdir(full_lib_dir):
        m = r.match(f)
        if m:
            os.symlink(f, path.join(".", full_lib_dir, m.group(1)))

    # Delete misc files (notices, licenses, etc.)
    for dir in [INCLUDE_DIR, LIB_DIR]:
        for g in ["LICENSE*", "RequiredVersion.txt", "ThirdPartyNotices.txt"]:
            for f in glob(path.join(dir, g)):
                os.remove(f)

    # Create autogenerated READMEs with timestamps
    for dir in [INCLUDE_DIR, LIB_DIR]:
        with open(f"{dir}/README.txt", "w") as f:
            f.write(f"Files downloaded by ../install_deps.py on {datetime.now()}")


def install_dep(maven_url, group_id, artifact_id, version, static, headers):
    """Download a dependency headers and static library."""

    url = urljoin(maven_url, group_id.replace('.', '/'), artifact_id, version)
    name = f"{artifact_id}-{version}"

    if headers:
        download_and_unzip(urljoin(url, f"{name}-headers.zip"), INCLUDE_DIR, name)

    download_and_unzip(
        urljoin(url, f"{name}-linuxathenastatic.zip" if static else f"{name}-linuxathena.zip"),
        LIB_DIR,
        name
    )


def download_and_unzip(url, dir, name):
    """Download a specificed zip file and unzip it."""

    full_path = path.join(dir, f"{name}.zip")

    print(f"Downloading {url}")
    try:
        request.urlretrieve(url, full_path)
        shutil.unpack_archive(full_path, dir)
        os.remove(full_path)
    except:
        sys.exit(f"Unable to download {url} - stopping")


def urljoin(*components):
    """Join url path components together."""

    components = filter(lambda c: c, components)
    return '/'.join([c.strip('/') for c in components])


if __name__ == "__main__":
    main()
